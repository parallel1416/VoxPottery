{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbb0c1d-5cb0-4e72-90ec-d047911ec298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data initialized\n",
      "VAE initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 257\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[38;5;66;03m# test()\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 257\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 196\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAE initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m### Call dataloader for train and test dataset\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m testloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dtest, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m### Implement GAN Loss!!\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "## Complete training and testing function for your 3D Voxel GAN and have fun making pottery art!\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "from utils.FragmentDataset import FragmentDataset\n",
    "import click\n",
    "from utils.model_utils import *\n",
    "import argparse\n",
    "from test import *\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, n_out, resolution=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # initialize superior inherited class, necessary hyperparams and modules\n",
    "        # You may use torch.nn.Conv3d(), torch.nn.sequential(), torch.nn.BatchNorm3d() for blocks\n",
    "        # You may try different activation functions such as ReLU or LeakyReLU.\n",
    "        # REMEMBER YOU ARE WRITING A DISCRIMINATOR (binary classification) so Sigmoid\n",
    "        self.scale = resolution // 32\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, 5, 1, 2),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(32, 32, 3, 2, 1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(32, 64, 3, 2, 1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.scale * self.scale * self.scale * 256, n_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Try to connect all modules to make the model operational!\n",
    "        # Note that the shape of x may need adjustment\n",
    "        # # Do not forget the batch size in x.dim\n",
    "        # TODO\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    # TODO\n",
    "    def __init__(self, n_labels, cube_len=64, z_latent_space=64, z_intern_space=64, device='cuda'):\n",
    "        super(Generator, self).__init__()\n",
    "        self.resolution = cube_len // 32\n",
    "        self.scale = (cube_len // 32) ** 3 * 1024  # dimensions of the final convolution result\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, 5, 1, 2),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(32, 32, 3, 2, 1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(32, 64, 3, 2, 1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(n_labels, 64),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * n_labels, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.flatten = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.scale, z_latent_space)\n",
    "        )\n",
    "        self.cat = lambda x, y: torch.cat((x, y), dim=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.scale, z_latent_space)\n",
    "        self.fc2 = nn.Linear(self.scale, z_latent_space)  # 1 and 2 for VI method\n",
    "        self.restore = nn.Sequential(\n",
    "            nn.Linear(z_latent_space + n_labels, z_latent_space),\n",
    "            nn.Linear(z_latent_space, self.scale)\n",
    "        )  # restoration of the mix layer ready for deconvolution\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, 3, 2, 1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, 3, 2, 1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 32, 5, 2, 2),\n",
    "\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = torch.randn(mean.shape).to(self.device)\n",
    "        z = mean + eps * torch.exp(logvar)\n",
    "        return z\n",
    "\n",
    "    def forward_encode(self, x):\n",
    "        z = self.encoder(x) # 2*2*2*256 (for 64)\n",
    "        mean = self.fc1(z.view(z.shape[0], -1))\n",
    "        logvar = self.fc2(z.view(z.shape[0], -1))\n",
    "        y = self.embedding(x)  # labels embedding layer\n",
    "        mix = self.flatten(z)\n",
    "        mix = self.cat(mix, y)\n",
    "        mix = self.restore(mix).view(-1, self.resolution, self.resolution, self.resolution, 256)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        z = self.cat(mix, z)\n",
    "        return z, mean, logvar\n",
    "\n",
    "    def forward_decode(self, x):\n",
    "        out = self.decoder(x)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.forward_encode(x)\n",
    "        out = self.forward_decode(z)\n",
    "        return out\n",
    "\n",
    "\n",
    "def CVAE_loss(z, x, mean, logstd, ratio):\n",
    "    MSEcriterion = nn.MSELoss().to(available_device)\n",
    "    mse = MSEcriterion(x, z)\n",
    "    var = torch.pow(torch.exp(logstd), 2)\n",
    "    kld = -0.5 * torch.sum(1 + torch.log(var) - torch.pow(mean, 2) - var)\n",
    "    return mse + kld * ratio\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Here is a simple demonstration argparse, you may customize your own implementations, and\n",
    "    # your hyperparam list MAY INCLUDE:\n",
    "    # 1. Z_latent_space\n",
    "    # 2. G_lr\n",
    "    # 3. D_lr  (learning rate for Discriminator)\n",
    "    # 4. betas if you are going to use Adam optimizer\n",
    "    # 5. Resolution for input data\n",
    "    # 6. Training Epochs\n",
    "    # 7. Test per epoch\n",
    "    # 8. Batch Size\n",
    "    # 9. Dataset Dir\n",
    "    # 10. Load / Save model Device\n",
    "    # 11. test result save dir\n",
    "    # 12. device!\n",
    "    # .... (maybe there exists more hyperparams to be appointed)\n",
    "    epochs = 100\n",
    "    G_lr = 2e-3\n",
    "    D_lr = 2e-4\n",
    "    C_lr = 2e-4\n",
    "    optimizer = 'ADAM'\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    batch_size = 64  # modify according to device capability\n",
    "    n_labels = 11\n",
    "    resolution = 32\n",
    "    z_latent_space = 1024\n",
    "    log_interval = 100\n",
    "    vi_ratio = 1\n",
    "\n",
    "    dirdataset = \"../data_voxelized\"\n",
    "    available_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--mode', type=str, help='training/testing')\n",
    "    parser.add_argument('-r', type=int, help='resolution')\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    ### Initialize train and test dataset\n",
    "    dtrain = FragmentDataset(dirdataset, 'train', resolution)\n",
    "    dtest = FragmentDataset(dirdataset, 'test', resolution)\n",
    "    print(\"Data initialized\")\n",
    "\n",
    "    ### Initialize Generator and Discriminator to specific device\n",
    "    G = Generator(n_labels, resolution, z_latent_space).to(available_device)\n",
    "    D = Discriminator(1, resolution).to(available_device)\n",
    "    C = Discriminator(n_labels, resolution).to(available_device)\n",
    "    optimG = optim.Adam(G.parameters(), G_lr, (beta1, beta2))\n",
    "    optimD = optim.Adam(D.parameters(), D_lr, (beta1, beta2))\n",
    "    optimC = optim.Adam(C.parameters(), C_lr, (beta1, beta2))\n",
    "    print(\"VAE initialized\")\n",
    "\n",
    "    ### Call dataloader for train and test dataset\n",
    "    trainloader = torch.utils.data.DataLoader(dtrain, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(dtest, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    ### Implement GAN Loss!!\n",
    "    # TODO\n",
    "    criterion = nn.BCELoss().to(available_device)  # BCE loss\n",
    "    # loss_function = 'BCE'\n",
    "\n",
    "    ### Training Loop implementation\n",
    "    ### You can refer to other papers / github repos for training a GAN\n",
    "    # TODO\n",
    "    print(\"Start training\")\n",
    "    for epoch in range(epochs):\n",
    "        for i, (vox, frg, label) in enumerate(trainloader, 0):\n",
    "            vox = vox.to(available_device)\n",
    "            frg = frg.to(available_device)\n",
    "            whole = vox+frg\n",
    "            label_onehot = torch.zeros((vox.shape[0], n_labels)).to(available_device)\n",
    "            label_onehot[torch.arange(vox.shape[0]), label] = 1\n",
    "            # train classifier on 11 types of ceramics (prepare for conditional GAN)\n",
    "            out = C(whole)\n",
    "            truth = label_onehot.to(available_device)\n",
    "            lossC = criterion(out, truth)\n",
    "            C.zero_grad()\n",
    "            lossC.backward()\n",
    "            optimC.step()\n",
    "            # train Discriminator\n",
    "            out = D(whole)\n",
    "            real_label = torch.ones(batch_size).to(available_device)  # real pieces labelled 1\n",
    "            fake_label = torch.zeros(batch_size).to(available_device)  # fake pieces labelled 0\n",
    "            lossD_real = criterion(out, real_label)\n",
    "\n",
    "            z = torch.randn(batch_size, z_latent_space + n_labels).to(available_device)\n",
    "            fake_data = G.forward_decode(z)+vox\n",
    "            out = D(fake_data)\n",
    "            lossD_fake = criterion(out, fake_label)\n",
    "\n",
    "            lossD = lossD_real + lossD_fake\n",
    "            D.zero_grad()\n",
    "            lossD.backward()\n",
    "            optimD.step()\n",
    "            # train Generator\n",
    "            z, mean, logstd = G.forward_encode(vox)\n",
    "            recon_data = G.forward_decode(z)\n",
    "            lossG_var_completion = CVAE_loss(recon_data, vox, mean, logstd, vi_ratio)\n",
    "            out = D(recon_data+vox)\n",
    "            truth = torch.ones(batch_size).to(available_device)\n",
    "            lossG_dis = criterion(out, truth)\n",
    "            out = C(recon_data+vox)\n",
    "            truth = label_onehot\n",
    "            lossG_condition = criterion(out, truth)\n",
    "            G.zero_grad()\n",
    "            lossG = lossG_var_completion + lossG_dis + lossG_condition\n",
    "            lossG.backward()\n",
    "            optimG.step()\n",
    "            if i % log_interval == 0:\n",
    "                print(\"i =\", i)\n",
    "                # test()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73f2ba-b3a5-465a-bb98-411ae8cd1202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
