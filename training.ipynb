{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cbb0c1d-5cb0-4e72-90ec-d047911ec298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data initialized\n",
      "VAE initialized\n",
      "Start training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 5, 5, 5], expected input[1, 64, 32, 32, 32] to have 1 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 258\u001b[0m\n\u001b[0;32m    254\u001b[0m                 \u001b[38;5;66;03m# test()\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 258\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 217\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    215\u001b[0m label_onehot[torch\u001b[38;5;241m.\u001b[39marange(vox\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# train classifier on 11 types of ceramics (prepare for conditional GAN)\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhole\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m truth \u001b[38;5;241m=\u001b[39m label_onehot\u001b[38;5;241m.\u001b[39mto(available_device)\n\u001b[0;32m    219\u001b[0m lossC \u001b[38;5;241m=\u001b[39m criterion(out, truth)\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Try to connect all modules to make the model operational!\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Note that the shape of x may need adjustment\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# # Do not forget the batch size in x.dim\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    607\u001b[0m     )\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 5, 5, 5], expected input[1, 64, 32, 32, 32] to have 1 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "## Complete training and testing function for your 3D Voxel GAN and have fun making pottery art!\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "from utils.FragmentDataset import FragmentDataset\n",
    "import click\n",
    "from utils.model_utils import *\n",
    "import argparse\n",
    "from test import *\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, n_out, resolution=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # initialize superior inherited class, necessary hyperparams and modules\n",
    "        # You may use torch.nn.Conv3d(), torch.nn.sequential(), torch.nn.BatchNorm3d() for blocks\n",
    "        # You may try different activation functions such as ReLU or LeakyReLU.\n",
    "        # REMEMBER YOU ARE WRITING A DISCRIMINATOR (binary classification) so Sigmoid\n",
    "        self.scale = resolution // 32\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, 5, 1, 2),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(32, 32, 3, 2, 1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(32, 64, 3, 2, 1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.scale * self.scale * self.scale * 256, n_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Try to connect all modules to make the model operational!\n",
    "        # Note that the shape of x may need adjustment\n",
    "        # # Do not forget the batch size in x.dim\n",
    "        # TODO\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    # TODO\n",
    "    def __init__(self, n_labels, cube_len=64, z_latent_space=64, z_intern_space=64, device='cuda'):\n",
    "        super(Generator, self).__init__()\n",
    "        self.resolution = cube_len // 32\n",
    "        self.scale = (cube_len // 32) ** 3 * 1024  # dimensions of the final convolution result\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, 5, 1, 2),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(32, 32, 3, 2, 1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(32, 64, 3, 2, 1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv3d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(n_labels, 64),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * n_labels, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.flatten = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.scale, z_latent_space)\n",
    "        )\n",
    "        self.cat = lambda x, y: torch.cat((x, y), dim=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.scale, z_latent_space)\n",
    "        self.fc2 = nn.Linear(self.scale, z_latent_space)  # 1 and 2 for VI method\n",
    "        self.restore = nn.Sequential(\n",
    "            nn.Linear(z_latent_space + n_labels, z_latent_space),\n",
    "            nn.Linear(z_latent_space, self.scale)\n",
    "        )  # restoration of the mix layer ready for deconvolution\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, 3, 2, 1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, 3, 2, 1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 32, 5, 2, 2),\n",
    "\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = torch.randn(mean.shape).to(self.device)\n",
    "        z = mean + eps * torch.exp(logvar)\n",
    "        return z\n",
    "\n",
    "    def forward_encode(self, x):\n",
    "        z = self.encoder(x) # 2*2*2*256 (for 64)\n",
    "        mean = self.fc1(z.view(z.shape[0], -1))\n",
    "        logvar = self.fc2(z.view(z.shape[0], -1))\n",
    "        y = self.embedding(x)  # labels embedding layer\n",
    "        mix = self.flatten(z)\n",
    "        mix = self.cat(mix, y)\n",
    "        mix = self.restore(mix).view(-1, self.resolution, self.resolution, self.resolution, 256)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        z = self.cat(mix, z)\n",
    "        return z, mean, logvar\n",
    "\n",
    "    def forward_decode(self, x):\n",
    "        out = self.decoder(x)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.forward_encode(x)\n",
    "        out = self.forward_decode(z)\n",
    "        return out\n",
    "\n",
    "\n",
    "def CVAE_loss(z, x, mean, logstd, ratio):\n",
    "    MSEcriterion = nn.MSELoss().to(available_device)\n",
    "    mse = MSEcriterion(x, z)\n",
    "    var = torch.pow(torch.exp(logstd), 2)\n",
    "    kld = -0.5 * torch.sum(1 + torch.log(var) - torch.pow(mean, 2) - var)\n",
    "    return mse + kld * ratio\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Here is a simple demonstration argparse, you may customize your own implementations, and\n",
    "    # your hyperparam list MAY INCLUDE:\n",
    "    # 1. Z_latent_space\n",
    "    # 2. G_lr\n",
    "    # 3. D_lr  (learning rate for Discriminator)\n",
    "    # 4. betas if you are going to use Adam optimizer\n",
    "    # 5. Resolution for input data\n",
    "    # 6. Training Epochs\n",
    "    # 7. Test per epoch\n",
    "    # 8. Batch Size\n",
    "    # 9. Dataset Dir\n",
    "    # 10. Load / Save model Device\n",
    "    # 11. test result save dir\n",
    "    # 12. device!\n",
    "    # .... (maybe there exists more hyperparams to be appointed)\n",
    "    epochs = 100\n",
    "    G_lr = 2e-3\n",
    "    D_lr = 2e-4\n",
    "    C_lr = 2e-4\n",
    "    optimizer = 'ADAM'\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    batch_size = 64  # modify according to device capability\n",
    "    n_labels = 11\n",
    "    resolution = 32\n",
    "    z_latent_space = 1024\n",
    "    log_interval = 100\n",
    "    vi_ratio = 1\n",
    "\n",
    "    dirdataset = \"../VoxPottery\"\n",
    "    available_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--mode', type=str, help='training/testing')\n",
    "    parser.add_argument('-r', type=int, help='resolution')\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    ### Initialize train and test dataset\n",
    "    dtrain = FragmentDataset(dirdataset, 'train', resolution)\n",
    "    dtest = FragmentDataset(dirdataset, 'test', resolution)\n",
    "    print(\"Data initialized\")\n",
    "\n",
    "    ### Initialize Generator and Discriminator to specific device\n",
    "    G = Generator(n_labels, resolution, z_latent_space).to(available_device)\n",
    "    D = Discriminator(1, resolution).to(available_device)\n",
    "    C = Discriminator(n_labels, resolution).to(available_device)\n",
    "    optimG = optim.Adam(G.parameters(), G_lr, (beta1, beta2))\n",
    "    optimD = optim.Adam(D.parameters(), D_lr, (beta1, beta2))\n",
    "    optimC = optim.Adam(C.parameters(), C_lr, (beta1, beta2))\n",
    "    print(\"VAE initialized\")\n",
    "\n",
    "    ### Call dataloader for train and test dataset\n",
    "    trainloader = torch.utils.data.DataLoader(dtrain, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(dtest, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    ### Implement GAN Loss!!\n",
    "    # TODO\n",
    "    criterion = nn.BCELoss().to(available_device)  # BCE loss\n",
    "    # loss_function = 'BCE'\n",
    "\n",
    "    ### Training Loop implementation\n",
    "    ### You can refer to other papers / github repos for training a GAN\n",
    "    # TODO\n",
    "    print(\"Start training\")\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            frg, vox, label = data\n",
    "            vox = vox.to(available_device)\n",
    "            frg = frg.to(available_device)\n",
    "            whole = vox + frg\n",
    "            label_onehot = torch.zeros((vox.shape[0], n_labels)).to(available_device)\n",
    "            label_onehot[torch.arange(vox.shape[0]), label] = 1\n",
    "            # train classifier on 11 types of ceramics (prepare for conditional GAN)\n",
    "            out = C(whole)\n",
    "            truth = label_onehot.to(available_device)\n",
    "            lossC = criterion(out, truth)\n",
    "            C.zero_grad()\n",
    "            lossC.backward()\n",
    "            optimC.step()\n",
    "            # train Discriminator\n",
    "            out = D(whole)\n",
    "            real_label = torch.ones(batch_size).to(available_device)  # real pieces labelled 1\n",
    "            fake_label = torch.zeros(batch_size).to(available_device)  # fake pieces labelled 0\n",
    "            lossD_real = criterion(out, real_label)\n",
    "\n",
    "            z = torch.randn(batch_size, z_latent_space + n_labels).to(available_device)\n",
    "            fake_data = G.forward_decode(z)+vox\n",
    "            out = D(fake_data)\n",
    "            lossD_fake = criterion(out, fake_label)\n",
    "\n",
    "            lossD = lossD_real + lossD_fake\n",
    "            D.zero_grad()\n",
    "            lossD.backward()\n",
    "            optimD.step()\n",
    "            # train Generator\n",
    "            z, mean, logstd = G.forward_encode(vox)\n",
    "            recon_data = G.forward_decode(z)\n",
    "            lossG_var_completion = CVAE_loss(recon_data, vox, mean, logstd, vi_ratio)\n",
    "            out = D(recon_data+vox)\n",
    "            truth = torch.ones(batch_size).to(available_device)\n",
    "            lossG_dis = criterion(out, truth)\n",
    "            out = C(recon_data+vox)\n",
    "            truth = label_onehot\n",
    "            lossG_condition = criterion(out, truth)\n",
    "            G.zero_grad()\n",
    "            lossG = lossG_var_completion + lossG_dis + lossG_condition\n",
    "            lossG.backward()\n",
    "            optimG.step()\n",
    "            if i % log_interval == 0:\n",
    "                print(\"i =\", i)\n",
    "                # test()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c73f2ba-b3a5-465a-bb98-411ae8cd1202",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([16, 2])) that is different to the input size (torch.Size([16])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m b_one_hot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(b\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Assuming 2 classes, change num_classes accordingly\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate cross-entropy loss\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mD:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3087\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3092\u001b[0m     )\n\u001b[0;32m   3094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([16, 2])) that is different to the input size (torch.Size([16])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define tensor a\n",
    "a = torch.tensor([0.4835, 0.5407, 0.4840, 0.5245, 0.5569, 0.6447, 0.5125, 0.4295, 0.5018,\n",
    "        0.6360, 0.5395, 0.5377, 0.5086, 0.5101, 0.4061, 0.5847])\n",
    "\n",
    "# Define tensor b\n",
    "b = torch.zeros(16)  # Create a tensor of zeros with dimension 16\n",
    "\n",
    "# Convert tensor b to one-hot encoding\n",
    "b_one_hot = F.one_hot(b.to(torch.int64), num_classes=2)  # Assuming 2 classes, change num_classes accordingly\n",
    "\n",
    "# Calculate cross-entropy loss\n",
    "loss = F.binary_cross_entropy(torch.sigmoid(a), b_one_hot.float())\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9068b-3057-45bd-a0dd-f218364d2156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
